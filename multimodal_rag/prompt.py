# from extract import extract_content
from retriever import create_retriever
from decodeencode import is_base64, split_image_text_types
import base64
from operator import itemgetter
from langchain_core.messages import HumanMessage , SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
import os 
import sys 
from extract import extract_text_elements
from extract import extract_and_save_images
import os

__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

pdf_path = "/home/vqa/masterthesis/ourproject/multimodal_rag/extracted_data/LLaVA.pdf"
path = '/home/vqa/masterthesis/ourproject/multimodal_rag/extracted_data/'

table_elements, text_elements, image_text_elements = extract_text_elements(pdf_path)
extract_and_save_images(pdf_path, path)

texts = []
for dictionary in text_elements + image_text_elements:
    if 'text' in dictionary:
        texts.append(dictionary['text'])


retriever = create_retriever(path, texts) 

def prompt_func(data_dict):
    # Joining the context texts into a single string
    formatted_texts = "\n".join(data_dict["context"]["texts"])
    # print(formatted_texts)
    messages = []

    # Adding image(s) to the messages if present
    if data_dict["context"]["images"]:
        image_message = {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{data_dict['context']['images'][0]}"
            },
        }
        # image_url = f"data:image/jpeg;base64,{data_dict['context']['images'][0]}"
        # image_message = {
        #     "type": "image_url",
        #     "image_url": image_url,
        # }
        messages.append(image_message)

    # Adding the text message for analysis
    text_message = {
        "type": "text",
        "text": (
            "Provide a detailed answer to the user question based on the provided context."
            f"User question: {data_dict['question']}\n\n"
            "Context:\n"
            f"{formatted_texts}"
        ),
    }

    messages.append(text_message)
    return [HumanMessage(content=messages)]


model = ChatOllama(model="llava")


# RAG pipeline
chain = (
    {
        "context": retriever | RunnableLambda(split_image_text_types),
        "question": RunnablePassthrough(),
    }
    | RunnableLambda(prompt_func)
    | model
    | StrOutputParser()
)


from IPython.display import HTML, display


def plt_img_base64(img_base64):
    # Create an HTML img tag with the base64 string as the source
    image_html = f'<img src="data:image/jpeg;base64,{img_base64}" />'

    # Display the image by rendering the HTML
    display(HTML(image_html))


docs = retriever.get_relevant_documents("chicken nuggets", k=3)
print('length', len(docs))


for doc in docs:
    doc_id = doc.metadata.get('doc_id', 'Unknown ID')
    print(f"Document ID: {doc_id}")



# DO NOT DELETE 
# for doc in docs:
#     page = doc.page_content
#     print('this....', doc.page_content)
#     if is_base64(doc.page_content):
#         plt_img_base64(doc.page_content)
#     # else:
#     #     print(doc.page_content)
# print(chain.invoke("Explain the chicken nugget meme."))




EVALUATION
from sklearn.metrics import precision_score, recall_score, f1_score

# Example pseudo-code
true_relevant_docs = [...]  # Ground truth relevant documents for each question
retrieved_docs = [...]  # Documents retrieved by the model for each question

precision = precision_score(true_relevant_docs, retrieved_docs, average='macro')
recall = recall_score(true_relevant_docs, retrieved_docs, average='macro')
f1 = f1_score(true_relevant_docs, retrieved_docs, average='macro')


from datasets import load_metric

# Example pseudo-code
bleu = load_metric("bleu")
exact_match = load_metric("accuracy")
f1 = load_metric("f1")

generated_answers = [...]  # Answers generated by the model
true_answers = [...]  # Ground truth answers

bleu_score = bleu.compute(predictions=generated_answers, references=true_answers)
exact_match_score = exact_match.compute(predictions=generated_answers, references=true_answers)
f1_score = f1.compute(predictions=generated_answers, references=true_answers)

